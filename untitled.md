## Comprehensive Review of the ETL Project Notebook for Rocket Elevator Data

### **Author:** Gavriel Rudolph  
### **Course:** AIC-102 Machine Learning 1: ETL  
### **Date:** January 24, 2025

---

### **1. Overall Structure and Organization**

The notebook is well-structured, following a logical flow from data loading and exploration to specific analyses of the `license.csv` dataset. The use of a Table of Contents with anchor links enhances navigability, allowing readers to quickly access different sections. Each major section is broken down into sub-sections, providing clarity and focus on specific tasks.

### **2. Code Quality and Readability**

- **Positive Aspects:**
  - **Modular Code:** Functions like `standardize_column_names` and `check_device_consistency` promote reusability and modularity.
  - **Clear Comments:** Most code blocks are accompanied by descriptive comments, explaining the purpose and methodology.
  - **Use of Libraries:** Appropriate libraries such as `pandas`, `numpy`, `matplotlib`, `seaborn`, and `missingno` are imported and utilized effectively.
  - **Consistent Naming Conventions:** Variables like `license_df`, `inspection_df`, etc., follow a consistent naming pattern, enhancing readability.

- **Areas for Improvement:**
  - **Error Handling:** The `Visualizing Missing Data` section encountered a `ValueError` due to an unrecognized keyword. Implementing error handling can prevent abrupt notebook interruptions.
  - **Consistent Variable Usage:** After filtering, both `license_df` and `filtered_license_df` are used interchangeably, leading to potential confusion. Maintaining consistency is crucial.
  - **Incomplete Code Blocks:** There are several empty code cells (` ```python ``` `) that should be either completed or removed to maintain professionalism.
  - **Date Parsing Warnings:** The `pd.to_datetime` function raises a warning about inferring formats. Specifying the date format can enhance parsing accuracy and suppress warnings.

### **3. Data Loading and Validation**

- **Positive Aspects:**
  - **Library Imports:** Essential libraries are imported at the beginning, and plot styles are set for better visualization.
  - **Data Verification:** Listing files in the directory ensures that the required data files are present before attempting to load them.

- **Areas for Improvement:**
  - **File Paths and Availability:** The file listing output does not display the data files (`license.csv`, `inspection.csv`, etc.), suggesting potential issues with file paths. Ensuring correct paths and verifying file existence is essential.
  - **Error Handling:** Incorporating try-except blocks when reading files can handle scenarios where files are missing or corrupted gracefully.

### **4. Data Exploration and Cleaning**

- **Positive Aspects:**
  - **Missing Values Identification:** The notebook effectively identifies missing values across different datasets.
  - **Descriptive Statistics:** Providing descriptive statistics offers insights into data distribution and potential anomalies.
  - **Column Standardization:** Standardizing column names across datasets ensures consistency, facilitating seamless merging and analysis.

- **Areas for Improvement:**
  - **Missing Data Visualization:** The `missingno` visualization encountered errors. Addressing these issues can provide better insights into missing data patterns.
  - **Handling Missing Data:** The approach to handling missing data varies (dropping vs. filling). Establishing a clear strategy based on data significance is recommended.
  - **Date Parsing Consistency:** Handling mixed date formats requires specifying the format explicitly to avoid parsing errors and warnings.

### **5. Specific Analysis of `license.csv`**

- **Positive Aspects:**
  - **Unique Identifier Verification:** Ensuring that `ElevatingDevicesNumber` uniquely identifies elevators across datasets is crucial for accurate merging and analysis.
  - **Location Extraction:** Extracting `State_Province` and `Country` from location strings enables geographical analysis.
  - **Filtering by License Status:** Focusing on 'ACTIVE' licenses refines the dataset to relevant records.

- **Areas for Improvement:**
  - **Consistency Checks:** The location consistency check reports high matches but also indicates discrepancies. Addressing these inconsistencies can improve data integrity.
  - **Visualization Enhancements:** Ensuring that plots render correctly and providing more contextual information can enhance interpretability.
  - **Final Data Saving:** Verifying the correctness of the saved cleaned data ensures that subsequent analyses are based on accurate datasets.

### **6. Discussions, Insights, and Notes**

- **Positive Aspects:**
  - **Key Takeaways:** Highlighting important observations and challenges provides valuable context.
  - **Reflection:** Emphasizing the importance of consistent data formats and step-by-step exploration underscores best practices in data analysis.

- **Areas for Improvement:**
  - **Depth of Insights:** Expanding on insights drawn from analyses, such as trends in license expirations or geographical distributions, can offer more value.
  - **Challenge Mitigation:** Detailing how specific challenges (e.g., location discrepancies) were resolved can enhance the notebook's instructional quality.

---

## **Checklist for Areas of Improvement**

1. **Data Loading Enhancements**
   - Verify and correct file paths.
   - Implement error handling when reading files.

2. **Missing Data Visualization Fixes**
   - Resolve `ValueError` in `missingno` visualizations.
   - Update plotting code to align with library updates.

3. **Column Standardization Improvements**
   - Ensure all relevant columns across datasets are standardized.
   - Automate column renaming to reduce redundancy.

4. **Date Parsing Accuracy**
   - Specify date formats in `pd.to_datetime` to handle mixed formats.
   - Suppress or handle parsing warnings appropriately.

5. **Consistent Variable Usage**
   - Use a single dataframe variable post-filtering (e.g., `filtered_license_df`).
   - Update all subsequent analyses to reference the correct dataframe.

6. **Error Handling and Robustness**
   - Incorporate try-except blocks where potential errors may occur.
   - Validate data after each transformation step.

7. **Code Cleanliness**
   - Remove incomplete or empty code cells.
   - Ensure all code cells are functional and necessary.

8. **Enhanced Documentation and Insights**
   - Expand on observations and insights from analyses.
   - Provide detailed explanations for steps taken to resolve data inconsistencies.

9. **Visualization Enhancements**
   - Ensure all plots render correctly without errors.
   - Add titles, labels, and legends where necessary for clarity.

10. **Final Data Saving Verification**
    - Confirm that the cleaned data is saved accurately.
    - Validate the saved files for completeness and correctness.

---

## **Updated Notebook Sections with Improvements**

Below are the updated sections addressing the identified areas of improvement. Each section includes improved markdown and Python code with comprehensive comments.

---

### **1. Setup and Data Loading**

#### **1.1 Importing Libraries**

```python
# Importing essential libraries
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import missingno as msno

# Setting plot style for better visuals
sns.set(style="whitegrid")

# Ensuring plots appear within the notebook
%matplotlib inline

# Suppress warnings for cleaner output
import warnings
warnings.filterwarnings('ignore')
```

#### **1.2 Loading Data**

```python
# Define the path to the data directory
data_path = '../data/raw/'

# List of expected data files
expected_files = ['license.csv', 'inspection.csv', 'installed.json', 'altered.json']

# Verify the presence of data files
print("Verifying data files in the directory:")
available_files = os.listdir(data_path)
print(available_files)

missing_files = [file for file in expected_files if file not in available_files]
if missing_files:
    print("\nMissing files:", missing_files)
else:
    print("\nAll expected data files are present.")

# Implement error handling when loading files
try:
    # Loading CSV data
    license_df = pd.read_csv(os.path.join(data_path, 'license.csv'))
    inspection_df = pd.read_csv(os.path.join(data_path, 'inspection.csv'))
    
    # Loading JSON data
    installed_df = pd.read_json(os.path.join(data_path, 'installed.json'))
    altered_df = pd.read_json(os.path.join(data_path, 'altered.json'))
    
    print("\nData files loaded successfully.")
except FileNotFoundError as e:
    print(f"Error: {e}. Please ensure all data files are present in the '{data_path}' directory.")
except pd.errors.EmptyDataError as e:
    print(f"Error: {e}. One of the files is empty.")
except Exception as e:
    print(f"An unexpected error occurred: {e}")
```

**Explanation of Improvements:**

- **File Path Verification:** Ensures that all expected data files are present before attempting to load them, preventing runtime errors.
- **Error Handling:** Implements try-except blocks to catch and inform about potential issues during file loading.
- **Dynamic File Path Construction:** Uses `os.path.join` for building file paths, enhancing compatibility across different operating systems.

---

### **2. Data Exploration**

#### **2.1 Previewing the Data**

```python
# Displaying the first few rows of each dataset
print("Preview of License Data:")
display(license_df.head())

print("\nPreview of Installed Data:")
display(installed_df.head())

print("\nPreview of Altered Data:")
display(altered_df.head())

print("\nPreview of Inspection Data:")
display(inspection_df.head())
```

#### **2.2 Checking for Missing Values**

```python
# Checking for missing values in each dataset
print("Missing Values in License Data:")
print(license_df.isnull().sum())

print("\nMissing Values in Installed Data:")
print(installed_df.isnull().sum())

print("\nMissing Values in Altered Data:")
print(altered_df.isnull().sum())

print("\nMissing Values in Inspection Data:")
print(inspection_df.isnull().sum())
```

#### **2.3 Descriptive Statistics**

```python
# Descriptive statistics for numerical columns in each dataset
print("Descriptive Statistics for License Data:")
display(license_df.describe())

print("\nDescriptive Statistics for Installed Data:")
display(installed_df.describe())

print("\nDescriptive Statistics for Altered Data:")
display(altered_df.describe())

print("\nDescriptive Statistics for Inspection Data:")
display(inspection_df.describe())
```

---

### **3. Visualizing Missing Data**

#### **3.1 Visualizing Missing Data**

```python
# Function to visualize missing data and handle potential errors
def visualize_missing_data(df, df_name):
    try:
        print(f"Missing Data Visualization for {df_name}:")
        msno.matrix(df, fontsize=12)
        plt.show()
    except Exception as e:
        print(f"Failed to visualize missing data for {df_name}. Error: {e}")

# Visualize missing data for each dataset
visualize_missing_data(license_df, "License Data")
visualize_missing_data(installed_df, "Installed Data")
visualize_missing_data(altered_df, "Altered Data")
visualize_missing_data(inspection_df, "Inspection Data")
```

**Explanation of Improvements:**

- **Error Handling:** Wraps the missing data visualization in a function with try-except blocks to catch and report errors without stopping the notebook execution.
- **Dynamic Function Usage:** Allows for reusability and cleaner code by using a dedicated function for visualization.

---

### **4. Analysis of the `license.csv` Dataset**

#### **4.1 Distinguishing Each Elevator**

##### **Objective**

Identify a unique variable to distinguish each elevator across datasets.

##### **Approach**

1. **Standardize Column Names:** Ensure that the column representing the elevator number is consistent across all datasets.
2. **Verify Uniqueness:** Check if the chosen identifier is unique within each dataset.
3. **Check Correspondence:** Ensure that device numbers correspond across datasets.

##### **Step 1: Define a Function to Standardize Column Names**

```python
# Define a function to standardize column names
def standardize_column_names(df, column_mappings):
    """
    Renames the columns of a dataframe based on the provided mapping.

    Parameters:
    - df (pd.DataFrame): The dataframe whose columns need to be renamed.
    - column_mappings (dict): A dictionary mapping old column names to new column names.
    """
    df.rename(columns=column_mappings, inplace=True)
```

##### **Step 2: Define Column Mappings for Each Dataset**

```python
# Define the column mappings for each dataset
license_column_mappings = {
    'ElevatingDevicesNumber': 'ElevatingDevicesNumber',
    'Elevating Devices Number': 'ElevatingDevicesNumber',
    'Elevating devices number': 'ElevatingDevicesNumber'
}

inspection_column_mappings = {
    'ElevatingDevicesNumber': 'ElevatingDevicesNumber',
    'Elevating Devices Number': 'ElevatingDevicesNumber',
    'Elevating devices number': 'ElevatingDevicesNumber'
}

installed_column_mappings = {
    'Elevating devices number': 'ElevatingDevicesNumber'
}

altered_column_mappings = {
    'Elevating Devices Number': 'ElevatingDevicesNumber',
    'Elevating devices number': 'ElevatingDevicesNumber',
    'Alteration  Location': 'AlterationLocation'
}
```

##### **Step 3: Standardize Column Names Across Datasets**

```python
# Standardize column names across datasets
standardize_column_names(license_df, license_column_mappings)
standardize_column_names(inspection_df, inspection_column_mappings)
standardize_column_names(installed_df, installed_column_mappings)
standardize_column_names(altered_df, altered_column_mappings)

print("Column names standardized across all datasets.")
```

##### **Step 4: Check Presence of the Common Identifier**

```python
# Check if 'ElevatingDevicesNumber' is present in all datasets
common_identifier = 'ElevatingDevicesNumber'
datasets = [license_df, inspection_df, installed_df, altered_df]
dataset_names = ['license_df', 'inspection_df', 'installed_df', 'altered_df']

for name, df in zip(dataset_names, datasets):
    if common_identifier in df.columns:
        print(f"'{common_identifier}' is present in {name}.")
    else:
        print(f"'{common_identifier}' is NOT present in {name}.")
```

##### **Step 5: Verify Uniqueness in Each Dataset**

```python
# Verify uniqueness of 'ElevatingDevicesNumber' in each dataset
for name, df in zip(dataset_names, datasets):
    is_unique = df[common_identifier].is_unique
    print(f"Is '{common_identifier}' unique in {name}? {is_unique}")
```

##### **Step 6: Check Correspondence of Device Numbers Across Datasets**

```python
# Check if the device numbers correspond across datasets
common_device_numbers = set(license_df[common_identifier])
for df in datasets[1:]:
    common_device_numbers &= set(df[common_identifier])

if len(common_device_numbers) > 0:
    print(f"Common device numbers found across all datasets: {len(common_device_numbers)}")
else:
    print("No common device numbers found across all datasets.")
```

##### **Step 7: Check Consistency of Device Locations Across Datasets**

```python
# Define a list of location-type columns in the same order as your datasets list.
# We have four DataFrames in this order: [license_df, inspection_df, installed_df, altered_df]
location_columns = [
    'LocationoftheElevatingDevice',  # license_df
    'InspectionLocation',            # inspection_df
    'Location of Device',            # installed_df
    'AlterationLocation'             # altered_df
]

# Define a function to check if the device number corresponds to the same elevator
def check_device_consistency(dfs, common_identifier, location_columns):
    """
    Merges datasets on the common identifier and checks the consistency of location columns.

    Parameters:
    - dfs (list of pd.DataFrame): List of dataframes to merge.
    - common_identifier (str): The column name used as the unique identifier.
    - location_columns (list of str): List of location column names corresponding to each dataframe.

    Returns:
    - merged_df (pd.DataFrame): The merged dataframe with consistency check results.
    """
    # Start with the first dataframe
    merged_df = dfs[0][[common_identifier, location_columns[0]]].copy()
    
    # Iteratively merge with the remaining dataframes
    for df, loc_col in zip(dfs[1:], location_columns[1:]):
        merged_df = merged_df.merge(df[[common_identifier, loc_col]], on=common_identifier, how='inner', suffixes=('', '_other'))
    
    # Check if the location columns are consistent
    for loc_col in location_columns[1:]:
        merged_df[f'location_match_{loc_col}'] = merged_df[location_columns[0]] == merged_df[loc_col]
    
    # Print the results
    for loc_col in location_columns[1:]:
        match_count = merged_df[f'location_match_{loc_col}'].sum()
        total_count = len(merged_df)
        print(f"Consistency check for {location_columns[0]} and {loc_col}: {match_count}/{total_count} matches")
    
    return merged_df

# Perform the consistency check
merged_df = check_device_consistency(datasets, common_identifier, location_columns)

# Display a sample of the merged DataFrame with consistency checks
display(merged_df.head())
```

**Explanation of Improvements:**

- **Function Documentation:** Provides clear docstrings explaining the purpose and parameters of functions.
- **Consistency Checks:** Reports the number of matching locations across datasets, highlighting data integrity.
- **Code Modularity:** Functions are defined for repetitive tasks, enhancing code reusability and clarity.

---

#### **4.2 Location Analysis**

##### **Objective**

Determine where the majority of elevators are located by extracting the country and state/province from the `LocationoftheElevatingDevice` column.

##### **Approach**

1. **Inspect the Data Format:** Ensure consistency in location strings.
2. **Extract Province and Country:** Use regular expressions with explicit formats.
3. **Handle Missing or Inconsistent Data:** Decide between dropping or filling missing values.
4. **Identify the Majority Location:** Use statistical methods to find the most common locations.

##### **Step 1: Inspect the Data Format**

```python
# Inspecting the 'LocationoftheElevatingDevice' column
print("Sample Locations:")
print(license_df['LocationoftheElevatingDevice'].head())
```

##### **Step 2: Extract Province and Country**

```python
# Define a regex pattern to extract province and country
# Assuming the format "Street Address City PostalCode Province Country"
# Example: "111 WELLESLEY ST W  TORONTO M7A 1A2 ON CA"
pattern = r'\b(\w{2})\s+(\w{2})$'

# Extract province and country into new columns
license_df[['State_Province', 'Country']] = license_df['LocationoftheElevatingDevice'].str.extract(pattern)

# Display the updated DataFrame
print(license_df[['LocationoftheElevatingDevice', 'State_Province', 'Country']].head())
```

##### **Step 3: Handle Missing or Inconsistent Data**

```python
# Check for rows where extraction failed
missing_locations = license_df[license_df['State_Province'].isnull() | license_df['Country'].isnull()]

print("Rows with missing or inconsistent location data:")
display(missing_locations)

# Option 1: Drop rows with missing values
license_df.dropna(subset=['State_Province', 'Country'], inplace=True)

# Option 2: Fill missing values (if preferable)
# license_df['State_Province'].fillna('Unknown', inplace=True)
# license_df['Country'].fillna('Unknown', inplace=True)

print(f"Total rows after handling missing location data: {license_df.shape[0]}")
```

##### **Step 4: Identify the Majority Location**

```python
# Determine the most common country and province
majority_country = license_df['Country'].mode()[0]
majority_state = license_df['State_Province'].mode()[0]

print(f"Most common country: {majority_country}")
print(f"Most common province: {majority_state}")

# Output a summary
print(f"The majority of elevators are located in {majority_state}, {majority_country}.")
```

##### **Notes and Insights**

- **Flexibility:** Using a regular expression with a specified pattern ensures accurate extraction as long as the data format remains consistent.
- **Handling Missing Data:** Dropping rows with missing location data maintains data integrity, especially if the number of such rows is minimal.
- **Geographical Focus:** The majority of elevators are located in Ontario, Canada (`ON, CA`), guiding future region-specific analyses.

---

#### **4.3 Filtering "LICENSESTATUS"**

##### **Objective**

Filter the dataset based on the `LICENSESTATUS` variable to focus on relevant records.

##### **Approach**

1. **Inspect Unique Statuses:** Understand the distribution of license statuses.
2. **Define Filtering Criteria:** Decide which statuses to retain (e.g., 'ACTIVE').
3. **Apply Filters:** Create a filtered dataframe based on the criteria.
4. **Summarize and Visualize:** Compare distributions before and after filtering.

##### **Step 1: Inspect Unique Statuses and Frequency**

```python
# Inspect unique LICENSESTATUS values and their counts
print("Unique LICENSESTATUS values:", license_df['LICENSESTATUS'].unique())
print(f"Total rows before filter: {license_df.shape[0]}")

# Visualize LICENSESTATUS distribution before filtering
plt.figure(figsize=(10, 6))
license_status_counts = license_df['LICENSESTATUS'].value_counts()
sns.barplot(x=license_status_counts.index, y=license_status_counts.values, palette='viridis')
plt.title('License Status Distribution (Before Filtering)', fontsize=16)
plt.xlabel('License Status', fontsize=12)
plt.ylabel('Count', fontsize=12)
plt.xticks(rotation=45, fontsize=10)
plt.tight_layout()
plt.show()
```

##### **Step 2: Define the Filtering Criteria**

```python
# Define the filtering criteria: retain only 'ACTIVE' licenses
relevant_status = ['ACTIVE']

# Filter the dataframe
filtered_license_df = license_df[license_df['LICENSESTATUS'].isin(relevant_status)].copy()

# Verify the filtering
print("LICENSESTATUS after filtering:")
print(filtered_license_df['LICENSESTATUS'].unique())
print(f"Number of records after filtering: {filtered_license_df.shape[0]}")
```

##### **Step 3: Handle Missing or Inconsistent Data (Optional)**

```python
# Check for missing values in LICENSESTATUS
missing_status_count = license_df['LICENSESTATUS'].isnull().sum()
print(f"Number of missing LICENSESTATUS values: {missing_status_count}")

# If there are missing values, decide on handling strategy
if missing_status_count > 0:
    # Option 1: Drop rows with missing LICENSESTATUS
    # filtered_license_df = filtered_license_df.dropna(subset=['LICENSESTATUS'])
    
    # Option 2: Fill missing values with 'Unknown'
    # filtered_license_df['LICENSESTATUS'].fillna('Unknown', inplace=True)
    
    print("Handled missing LICENSESTATUS values.")
else:
    print("No missing LICENSESTATUS values to handle.")
```

##### **Step 4: Summarize and Visualize the Filtered Data**

```python
# Summarizing the filtering process
total_rows_before = license_df.shape[0]
total_rows_after = filtered_license_df.shape[0]
rows_filtered_out = total_rows_before - total_rows_after

print(f"Total rows before filtering: {total_rows_before}")
print(f"Total rows after filtering: {total_rows_after}")
print(f"Rows filtered out: {rows_filtered_out}")

# Visualize LICENSESTATUS distribution after filtering
plt.figure(figsize=(6, 4))
filtered_status_counts = filtered_license_df['LICENSESTATUS'].value_counts()
sns.barplot(x=filtered_status_counts.index, y=filtered_status_counts.values, palette='magma')
plt.title('License Status Distribution (After Filtering)', fontsize=16)
plt.xlabel('License Status', fontsize=12)
plt.ylabel('Count', fontsize=12)
plt.tight_layout()
plt.show()
```

**Explanation of Improvements:**

- **Enhanced Visualization:** Utilizes `seaborn` for more aesthetically pleasing and informative plots.
- **Copying Dataframe:** Creates a copy of the filtered dataframe to prevent SettingWithCopy warnings and maintain data integrity.
- **Conditional Handling:** Checks for missing values before deciding on a handling strategy, ensuring flexibility based on data conditions.

---

#### **4.4 Verifying Uniqueness After Filtering**

##### **Objective**

Confirm if `ElevatingDevicesNumber` remains unique after filtering the dataset.

##### **Approach**

1. **Check Uniqueness:** Use the `.is_unique` property on the filtered dataframe.
2. **Interpret Results:** Ensure the identifier's uniqueness for reliable merging and analysis.

##### **Implementation**

```python
# Rechecking uniqueness after filtering
is_unique_after_filter = filtered_license_df[common_identifier].is_unique
print(f"Is '{common_identifier}' unique after filtering? {is_unique_after_filter}")
```

**Explanation of Improvements:**

- **Accurate Data Reference:** Uses `filtered_license_df` instead of `license_df` to reflect the dataset post-filtering.
- **Clear Output:** Provides a straightforward answer to the uniqueness verification.

---

#### **4.5 Time Series Plot of License Expiry**

##### **Objective**

Plot a time series showing the count of license expirations by month.

##### **Approach**

1. **Convert `LICENSEEXPIRYDATE` to Datetime:** Ensure accurate date parsing.
2. **Handle Mixed Date Formats:** Specify date formats to prevent parsing errors.
3. **Group and Plot Data:** Aggregate expirations by month and visualize trends.

##### **Implementation**

```python
# Convert LICENSEEXPIRYDATE to datetime with specified format
# Assuming the format is 'dd-MMM-yy' (e.g., '28-Apr-17')
license_expiry_format = '%d-%b-%y'

try:
    filtered_license_df['LICENSEEXPIRYDATE'] = pd.to_datetime(filtered_license_df['LICENSEEXPIRYDATE'], format=license_expiry_format)
    print("LICENSEEXPIRYDATE converted to datetime successfully.")
except ValueError as e:
    print(f"Date conversion error: {e}. Attempting to infer formats.")
    filtered_license_df['LICENSEEXPIRYDATE'] = pd.to_datetime(filtered_license_df['LICENSEEXPIRYDATE'], errors='coerce')
    print("Date conversion with inference completed.")

# Check for any NaT values after conversion
nat_count = filtered_license_df['LICENSEEXPIRYDATE'].isnull().sum()
print(f"Number of invalid date entries: {nat_count}")

# Drop rows with invalid dates if any
filtered_license_df.dropna(subset=['LICENSEEXPIRYDATE'], inplace=True)

# Extract month and year for grouping
filtered_license_df['ExpiryMonth'] = filtered_license_df['LICENSEEXPIRYDATE'].dt.month_name()
filtered_license_df['ExpiryYear'] = filtered_license_df['LICENSEEXPIRYDATE'].dt.year

# Group by year and month, then count expirations
expiry_counts = filtered_license_df.groupby(['ExpiryYear', 'ExpiryMonth']).size().reset_index(name='Count')

# Create a 'Year-Month' column for sorting
expiry_counts['YearMonth'] = pd.to_datetime(expiry_counts['ExpiryYear'].astype(str) + '-' + expiry_counts['ExpiryMonth'], format='%Y-%B')

# Sort by 'YearMonth'
expiry_counts.sort_values('YearMonth', inplace=True)

# Plot the time series
plt.figure(figsize=(14, 7))
sns.lineplot(data=expiry_counts, x='YearMonth', y='Count', marker='o', color='teal')
plt.title('License Expirations Over Time', fontsize=16)
plt.xlabel('Year-Month', fontsize=12)
plt.ylabel('Number of Expirations', fontsize=12)
plt.xticks(rotation=45)
plt.tight_layout()
plt.grid(True, linestyle='--', alpha=0.5)
plt.show()
```

**Explanation of Improvements:**

- **Specified Date Format:** Addresses the warning by specifying the date format, ensuring accurate parsing.
- **Error Handling:** Attempts to infer formats if initial parsing fails, ensuring maximum data retention.
- **Handling Invalid Dates:** Drops rows with invalid dates to maintain data integrity.
- **Enhanced Plotting:** Uses `seaborn` for a cleaner line plot with markers, improving visual appeal and clarity.

---

#### **4.6 Table of Expirations by Year-Month**

##### **Objective**

Create a table that counts expirations by year and month, formatted for readability, and filter for months with more than five expirations.

##### **Approach**

1. **Create Year-Month Column:** Format dates for aggregation.
2. **Aggregate Data:** Count expirations per year-month.
3. **Filter Based on Threshold:** Retain only records exceeding the specified count.

##### **Implementation**

```python
# Create the 'Expiry_YearMonth' column in 'YYYY-MM' format
filtered_license_df['Expiry_YearMonth'] = filtered_license_df['LICENSEEXPIRYDATE'].dt.to_period('M').astype(str)

# Counting expirations by year and month
expiry_table = filtered_license_df.groupby('Expiry_YearMonth').size().reset_index(name='Count')

# Filtering for counts > 5
expiry_table = expiry_table[expiry_table['Count'] > 5]

# Creating a readable 'Year-Month' format (e.g., January 2015)
expiry_table['Year-Month'] = pd.to_datetime(expiry_table['Expiry_YearMonth']).dt.strftime('%B %Y')

# Selecting and ordering relevant columns
expiry_table = expiry_table[['Expiry_YearMonth', 'Count', 'Year-Month']]

# Displaying the table
print("License Expirations by Year-Month (Count > 5):")
display(expiry_table)
```

**Explanation of Improvements:**

- **Clear Formatting:** The `Year-Month` column provides a more readable format for reports and presentations.
- **Efficient Filtering:** Utilizes vectorized operations for filtering, enhancing performance.
- **Structured Output:** Presents the table in a clear, concise manner, facilitating easy interpretation.

---

### **5. Final Data Saving**

```python
# Save the cleaned and filtered license data to a CSV file
output_file = 'cleaned_license_data.csv'
filtered_license_df.to_csv(output_file, index=False)
print(f"Cleaned license data saved to '{output_file}'.")
```

**Explanation of Improvements:**

- **Clear Output Message:** Informs the user about the successful saving of the cleaned data.
- **Parameterization:** Uses a variable for the output file name, allowing easy modifications.

---

### **6. Additional Recommendations**

1. **Automate Column Standardization:**
   - Create a comprehensive mapping for all datasets to avoid manual renaming and reduce errors.

2. **Implement Logging:**
   - Use Pythonâ€™s `logging` library to record the progress and issues during the ETL process.

3. **Modularize the Notebook:**
   - Break down the notebook into smaller, focused sections or separate notebooks for different ETL phases.

4. **Add Summary Sections:**
   - After major steps, include summary markdown cells to recap findings and decisions.

5. **Version Control:**
   - Use Git for version control to track changes and collaborate effectively.

6. **Unit Testing:**
   - Incorporate tests to verify the correctness of functions, especially for data transformations.

7. **Data Validation:**
   - Implement checks to ensure data integrity post-ETL, such as validating row counts and data types.

8. **Enhance Visualizations:**
   - Add interactive visualizations using libraries like `plotly` for more dynamic data exploration.

---

## **Conclusion**

The ETL project notebook demonstrates a solid foundation in data extraction, transformation, and loading processes. By addressing the identified areas of improvementâ€”ranging from error handling and code consistency to enhanced documentation and visualizationâ€”the notebook can be elevated to a more robust and professional standard. Implementing these recommendations will not only enhance the reliability and accuracy of the analyses but also improve the overall readability and maintainability of the code.

---

**Happy Coding! ðŸš€**